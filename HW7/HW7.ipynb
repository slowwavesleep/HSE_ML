{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW7.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flLpKhvYnGBp",
        "colab_type": "text"
      },
      "source": [
        "Сверточные сети относительно хорошо масшабируются на длинные последовательности. Поэтому их часто применяют к отдельным символам, а не токенам. В домашке вам нужно будет обучить большую сверточную модель на символах (отличаться по сути будет только токенизация).\n",
        "\n",
        "При обучении используйте колбек для отслеживания лучшей модели. Ориентируйтесь на ф1 меру.\n",
        "\n",
        "Конкретнее задание такое: Обучите модель с минимум 15 слоями, где у каждого слоя разные параметры (Dropout, Conv1d и Pooling считаются слоями, остальное нет). Как минимум 4 слоя должны быть наложены друг на друга. Должен быть хотя бы один слой каждого типа.\n",
        "\n",
        "Советы: Начните с небольших сетей и постепенно добавляйте, не пытайтесь сразу собрать все слои. Сделайте размер эмбединга сильно меньше. Попробуйте паддинг поменьше. Символьная модель может обучаться намного дольше. Иногда кернел может крашиться просто так или из-за слишком больших матриц.\n",
        "\n",
        "Бонусный балл можно получить за изучение влияния предобработки (нужно ли приводить к нижнему регистру, нужно ли выкидывать не алфавитные символы, помогает ли замена цифр на определенный токен)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GUoYGhgnGoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STyacS9vnWRQ",
        "colab_type": "code",
        "outputId": "9176da7f-5f67-4ce3-bebf-eb65d89715f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd 'drive/My Drive/colab_data'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/colab_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4jFn5R5nhaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "\n",
        "from string import ascii_lowercase, digits, ascii_letters, punctuation\n",
        "short_punctuation = ',.!?;'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGC1oZKYnj4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora = pd.read_csv('quora.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f58eU2UPnnK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred):\n",
        "  \n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK06P-fSdK6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars_to_remove = r\"[\\ufeff|\\u202c|\\x7f|\\u200b|\\xad|\\u2060|\\u200c|\\uf02d|\\x10|\\u200e|\\u2061]\"\n",
        "whitespace = r\"[\\t|\\n|\\r|\\x0b|\\x0c]\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rCsfakVqfMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def tokenize(text: str) -> list:\n",
        "#   tokens = text.lower().split()\n",
        "#   return [token.strip(punctuation) for token in tokens]\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(chars_to_remove, ' ', text) # funky punctuation symbols\n",
        "  text = re.sub(whitespace, ' ', text) # different symbols for whitespace\n",
        "  text = re.sub(r\" {2,}\", ' ', text) # multiple whitespaces\n",
        "  return text\n",
        "\n",
        "def tokenize(text: str) -> list:\n",
        "  result = []\n",
        "  for ch in text.lower():\n",
        "    if ch in ascii_lowercase or ch == ' ':\n",
        "      result.append(ch)\n",
        "    elif ch in digits:\n",
        "      result.append('DIG')\n",
        "    elif ch in short_punctuation:\n",
        "      result.append('PNC')\n",
        "    else:\n",
        "      result.append('UNK')\n",
        "  return result\n",
        "\n",
        "def filter_dict(d: dict, func: callable) -> dict:\n",
        "    new_d = dict()\n",
        "    for key, value in d.items():\n",
        "        if func((key, value)):\n",
        "            new_d[key] = value\n",
        "    return new_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fusra5sIuC-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(texts: list, min_count: int) -> set:\n",
        "  \n",
        "  vocab = Counter()\n",
        "\n",
        "  for text in texts:\n",
        "    vocab.update(text)\n",
        "\n",
        "  return set(filter_dict(vocab, lambda x: x[1] > min_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MdSW4ifuEaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_chars(chars: set) -> dict:\n",
        "  d = {'PNC': 3, 'DIG': 2, 'UNK': 1, 'PAD': 0}\n",
        "  for ch in chars :\n",
        "    if ch not in d.keys():\n",
        "      d[ch] = len(d)\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwxrTnTjuGoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_text(text: list) -> list:\n",
        "  return [char2id.get(token, 1) for token in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBZqZ6N5feAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora['cleaned'] = quora.question_text.apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtegLa1vuIOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora['tokenized'] = quora.cleaned.apply(tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipMIy6_3uM_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = build_vocab(quora.tokenized.values, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyUOQQjtuQG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2id = index_chars(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA-HXw6MuRgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2char = {i: ch for ch, i in char2id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whRDSBA4uS0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora['indexed'] = quora.tokenized.apply(index_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E97ij7i8Im21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def limit_max_len(len_list, n_sigma):\n",
        "  mean = np.mean(len_list)\n",
        "  std = np.std(len_list)\n",
        "  return int(np.ceil(mean + (n_sigma * std)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4LK7qZ1IiTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_limit = limit_max_len(quora.indexed.str.len(), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkjpZuycJar-",
        "colab_type": "code",
        "outputId": "78e5a556-215a-4c13-e610-d1edce751e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len_limit"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arDi6S7yuVOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.keras.preprocessing.sequence.pad_sequences(quora.indexed.values, maxlen=len_limit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4lEcnAWueRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = quora.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmgiIpoVufNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
        "                                                      test_size=0.05,\n",
        "                                                      random_state=1,\n",
        "                                                      stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuub7Y43NteI",
        "colab_type": "code",
        "outputId": "9f6dc5a4-ede4-4b64-9c6f-3b11b95f1a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1306122, 110)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE1iF3GfzQuS",
        "colab_type": "text"
      },
      "source": [
        "# Попытка 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipOtwiuwukJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(len_limit,))\n",
        "\n",
        "embeddings = tf.keras.layers.Embedding(input_dim=len(char2id), output_dim=50)(inputs)\n",
        "\n",
        "conv_1 = tf.keras.layers.Conv1D(kernel_size=3, filters=16, strides=1)(embeddings)\n",
        "conv_2 = tf.keras.layers.Conv1D(kernel_size=3, filters=16, strides=1)(conv_1)\n",
        "conv_3 = tf.keras.layers.Conv1D(kernel_size=3, filters=16, strides=1)(conv_2)\n",
        "pool_1 = tf.keras.layers.AveragePooling1D()(conv_3)\n",
        "drop_1 = tf.keras.layers.Dropout(0.1)(pool_1)\n",
        "\n",
        "conv_4 = tf.keras.layers.Conv1D(kernel_size=3, filters=16, strides=1)(drop_1)\n",
        "conv_5 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1)(conv_4)\n",
        "conv_6 = tf.keras.layers.Conv1D(kernel_size=3, filters=64, strides=1, activation='relu')(conv_5)\n",
        "pool_2 = tf.keras.layers.AveragePooling1D()(conv_6)\n",
        "drop_2 = tf.keras.layers.Dropout(0.1)(pool_2)\n",
        "\n",
        "conv_7 = tf.keras.layers.Conv1D(kernel_size=4, filters=16, strides=1, activation='relu')(drop_2)\n",
        "conv_8 = tf.keras.layers.Conv1D(kernel_size=4, filters=32, strides=1, activation='relu')(conv_7)\n",
        "conv_9 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='relu')(conv_8)\n",
        "pool_3 = tf.keras.layers.MaxPooling1D()(conv_9)\n",
        "drop_3 = tf.keras.layers.Dropout(0.1)(pool_3)\n",
        "\n",
        "concat = tf.keras.layers.Flatten()(drop_3)\n",
        "dense_1 = tf.keras.layers.Dense(64, activation='relu')(concat)\n",
        "dense_2 = tf.keras.layers.Dense(64, activation='relu')(dense_1)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_2)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-yJzOe5xoXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WctVkDTcxhum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.weights', \n",
        "                                                monitor='val_f1',\n",
        "                                                verbose=1,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                mode='max',\n",
        "                                                save_freq='epoch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-Lqs9alxm82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[f1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxDkQidywk-d",
        "colab_type": "code",
        "outputId": "406c1b6c-17bc-4e89-c122-8bd16f27cf6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=4000,\n",
        "          epochs=15,\n",
        "          callbacks=[checkpoint])"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.2083 - f1: 0.0000e+00\n",
            "Epoch 00001: val_f1 improved from -inf to 0.00000, saving model to model.weights\n",
            "311/311 [==============================] - 248s 799ms/step - loss: 0.2083 - f1: 0.0000e+00 - val_loss: 0.1951 - val_f1: 0.0000e+00\n",
            "Epoch 2/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1923 - f1: 6.1491e-04\n",
            "Epoch 00002: val_f1 improved from 0.00000 to 0.00317, saving model to model.weights\n",
            "311/311 [==============================] - 247s 793ms/step - loss: 0.1923 - f1: 6.1491e-04 - val_loss: 0.1835 - val_f1: 0.0032\n",
            "Epoch 3/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1820 - f1: 0.0728\n",
            "Epoch 00003: val_f1 improved from 0.00317 to 0.16109, saving model to model.weights\n",
            "311/311 [==============================] - 248s 797ms/step - loss: 0.1820 - f1: 0.0728 - val_loss: 0.1749 - val_f1: 0.1611\n",
            "Epoch 4/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1761 - f1: 0.1698\n",
            "Epoch 00004: val_f1 improved from 0.16109 to 0.23139, saving model to model.weights\n",
            "311/311 [==============================] - 248s 796ms/step - loss: 0.1761 - f1: 0.1698 - val_loss: 0.1719 - val_f1: 0.2314\n",
            "Epoch 5/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1721 - f1: 0.2155\n",
            "Epoch 00005: val_f1 did not improve from 0.23139\n",
            "311/311 [==============================] - 247s 793ms/step - loss: 0.1721 - f1: 0.2155 - val_loss: 0.1680 - val_f1: 0.2039\n",
            "Epoch 6/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1691 - f1: 0.2482\n",
            "Epoch 00006: val_f1 did not improve from 0.23139\n",
            "311/311 [==============================] - 250s 803ms/step - loss: 0.1691 - f1: 0.2482 - val_loss: 0.1660 - val_f1: 0.1952\n",
            "Epoch 7/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1669 - f1: 0.2632\n",
            "Epoch 00007: val_f1 improved from 0.23139 to 0.35665, saving model to model.weights\n",
            "311/311 [==============================] - 257s 827ms/step - loss: 0.1669 - f1: 0.2632 - val_loss: 0.1646 - val_f1: 0.3566\n",
            "Epoch 8/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1652 - f1: 0.2828\n",
            "Epoch 00008: val_f1 did not improve from 0.35665\n",
            "311/311 [==============================] - 252s 810ms/step - loss: 0.1652 - f1: 0.2828 - val_loss: 0.1620 - val_f1: 0.2573\n",
            "Epoch 9/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1635 - f1: 0.2889\n",
            "Epoch 00009: val_f1 did not improve from 0.35665\n",
            "311/311 [==============================] - 251s 807ms/step - loss: 0.1635 - f1: 0.2889 - val_loss: 0.1594 - val_f1: 0.3025\n",
            "Epoch 10/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1624 - f1: 0.2995\n",
            "Epoch 00010: val_f1 did not improve from 0.35665\n",
            "311/311 [==============================] - 255s 821ms/step - loss: 0.1624 - f1: 0.2995 - val_loss: 0.1591 - val_f1: 0.2932\n",
            "Epoch 11/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1610 - f1: 0.3108\n",
            "Epoch 00011: val_f1 did not improve from 0.35665\n",
            "311/311 [==============================] - 249s 802ms/step - loss: 0.1610 - f1: 0.3108 - val_loss: 0.1570 - val_f1: 0.3399\n",
            "Epoch 12/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1598 - f1: 0.3226\n",
            "Epoch 00012: val_f1 improved from 0.35665 to 0.40001, saving model to model.weights\n",
            "311/311 [==============================] - 250s 805ms/step - loss: 0.1598 - f1: 0.3226 - val_loss: 0.1577 - val_f1: 0.4000\n",
            "Epoch 13/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1593 - f1: 0.3243\n",
            "Epoch 00013: val_f1 did not improve from 0.40001\n",
            "311/311 [==============================] - 249s 802ms/step - loss: 0.1593 - f1: 0.3243 - val_loss: 0.1572 - val_f1: 0.3865\n",
            "Epoch 14/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1583 - f1: 0.3316\n",
            "Epoch 00014: val_f1 did not improve from 0.40001\n",
            "311/311 [==============================] - 249s 801ms/step - loss: 0.1583 - f1: 0.3316 - val_loss: 0.1564 - val_f1: 0.3927\n",
            "Epoch 15/15\n",
            "311/311 [==============================] - ETA: 0s - loss: 0.1573 - f1: 0.3403\n",
            "Epoch 00015: val_f1 did not improve from 0.40001\n",
            "311/311 [==============================] - 244s 786ms/step - loss: 0.1573 - f1: 0.3403 - val_loss: 0.1533 - val_f1: 0.3615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3b3a00c8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftcpi9pnzJf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5605d59b-66a2-4d61-8666-76e64ce1f162"
      },
      "source": [
        "model.load_weights('model.weights')"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3b33baa2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cIkI8pNCzz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(X_valid).reshape(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lEJV6XaDB4d",
        "colab_type": "code",
        "outputId": "d276f703-1a1f-450e-ad31-a787fbd32be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_valid, (preds > 0.5).astype(int)))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97     61266\n",
            "           1       0.61      0.29      0.40      4041\n",
            "\n",
            "    accuracy                           0.94     65307\n",
            "   macro avg       0.78      0.64      0.68     65307\n",
            "weighted avg       0.93      0.94      0.94     65307\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbvv9GJmzUmC",
        "colab_type": "text"
      },
      "source": [
        "# Попытка 2 (основная)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0g9u6M74DcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(len_limit,))\n",
        "\n",
        "embeddings = tf.keras.layers.Embedding(input_dim=len(char2id), output_dim=50)(inputs)\n",
        "\n",
        "conv = []\n",
        "\n",
        "conv_1 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_1)\n",
        "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_2)\n",
        "pool_1 = tf.keras.layers.GlobalMaxPooling1D()(conv_3)\n",
        "drop_1 = tf.keras.layers.AlphaDropout(0.1)(pool_1)\n",
        "conv.append(drop_1)\n",
        "\n",
        "conv_4 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(embeddings)\n",
        "conv_5 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_4)\n",
        "conv_6 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_5)\n",
        "pool_2 = tf.keras.layers.GlobalMaxPooling1D()(conv_6)\n",
        "drop_2 = tf.keras.layers.AlphaDropout(0.1)(pool_2)\n",
        "conv.append(drop_2)\n",
        "\n",
        "conv_7 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(embeddings)\n",
        "conv_8 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_7)\n",
        "conv_9 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_8)\n",
        "pool_3 = tf.keras.layers.GlobalMaxPooling1D()(conv_9)\n",
        "drop_3 = tf.keras.layers.AlphaDropout(0.1)(pool_3)\n",
        "conv.append(drop_3)\n",
        "\n",
        "conv_10 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_11 = tf.keras.layers.Conv1D(kernel_size=2, filters=32, strides=1, activation='tanh')(conv_10)\n",
        "conv_12 = tf.keras.layers.Conv1D(kernel_size=2, filters=64, strides=1, activation='tanh')(conv_11)\n",
        "conv_13 = tf.keras.layers.Conv1D(kernel_size=2, filters=128, strides=1, activation='tanh')(conv_12)\n",
        "pool_4 = tf.keras.layers.GlobalMaxPooling1D()(conv_13)\n",
        "drop_4 = tf.keras.layers.AlphaDropout(0.1)(pool_4)\n",
        "conv.append(drop_4)\n",
        "\n",
        "concat = tf.keras.layers.Concatenate()(conv)\n",
        "dense_1 = tf.keras.layers.Dense(128, activation='selu')(concat)\n",
        "dense_2 = tf.keras.layers.Dense(64, activation='selu')(dense_1)\n",
        "dense_3 = tf.keras.layers.Dense(32, activation='selu')(dense_2)\n",
        "drop_5 = tf.keras.layers.AlphaDropout(0.1)(dense_3)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(drop_5)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgm3mb4y0Je0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cdTfEVj0Mrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.weights', \n",
        "                                                monitor='val_f1',\n",
        "                                                verbose=1,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                mode='max',\n",
        "                                                save_freq='epoch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS5Gd4yC0O42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[f1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXzt1h680UjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a0e47ab-584c-48a1-8031-9ab52b742c85"
      },
      "source": [
        "model_2.fit(X_train, y_train, \n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=4000,\n",
        "          epochs=30,\n",
        "          callbacks=[checkpoint])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.2073 - f1: 3.5746e-04\n",
            "Epoch 00001: val_f1 improved from -inf to 0.00000, saving model to model.weights\n",
            "311/311 [==============================] - 40s 130ms/step - loss: 0.2073 - f1: 3.5631e-04 - val_loss: 0.1940 - val_f1: 0.0000e+00\n",
            "Epoch 2/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1797 - f1: 0.1490\n",
            "Epoch 00002: val_f1 improved from 0.00000 to 0.28512, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1798 - f1: 0.1491 - val_loss: 0.1654 - val_f1: 0.2851\n",
            "Epoch 3/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1614 - f1: 0.2903\n",
            "Epoch 00003: val_f1 improved from 0.28512 to 0.29826, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1613 - f1: 0.2907 - val_loss: 0.1675 - val_f1: 0.2983\n",
            "Epoch 4/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1554 - f1: 0.3316\n",
            "Epoch 00004: val_f1 improved from 0.29826 to 0.44185, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1553 - f1: 0.3317 - val_loss: 0.1529 - val_f1: 0.4418\n",
            "Epoch 5/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1513 - f1: 0.3592\n",
            "Epoch 00005: val_f1 improved from 0.44185 to 0.45842, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1514 - f1: 0.3589 - val_loss: 0.1513 - val_f1: 0.4584\n",
            "Epoch 6/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1482 - f1: 0.3826\n",
            "Epoch 00006: val_f1 did not improve from 0.45842\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1481 - f1: 0.3830 - val_loss: 0.1511 - val_f1: 0.4482\n",
            "Epoch 7/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1451 - f1: 0.3983\n",
            "Epoch 00007: val_f1 did not improve from 0.45842\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1450 - f1: 0.3986 - val_loss: 0.1536 - val_f1: 0.4029\n",
            "Epoch 8/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1421 - f1: 0.4195\n",
            "Epoch 00008: val_f1 improved from 0.45842 to 0.46119, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1421 - f1: 0.4195 - val_loss: 0.1421 - val_f1: 0.4612\n",
            "Epoch 9/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1396 - f1: 0.4331\n",
            "Epoch 00009: val_f1 did not improve from 0.46119\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1396 - f1: 0.4329 - val_loss: 0.1423 - val_f1: 0.4455\n",
            "Epoch 10/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1374 - f1: 0.4480\n",
            "Epoch 00010: val_f1 did not improve from 0.46119\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1374 - f1: 0.4479 - val_loss: 0.1463 - val_f1: 0.4597\n",
            "Epoch 11/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1352 - f1: 0.4580\n",
            "Epoch 00011: val_f1 did not improve from 0.46119\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1353 - f1: 0.4577 - val_loss: 0.1466 - val_f1: 0.4481\n",
            "Epoch 12/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1336 - f1: 0.4660\n",
            "Epoch 00012: val_f1 improved from 0.46119 to 0.47053, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1336 - f1: 0.4660 - val_loss: 0.1461 - val_f1: 0.4705\n",
            "Epoch 13/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1319 - f1: 0.4755\n",
            "Epoch 00013: val_f1 improved from 0.47053 to 0.48811, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1319 - f1: 0.4754 - val_loss: 0.1391 - val_f1: 0.4881\n",
            "Epoch 14/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1305 - f1: 0.4842\n",
            "Epoch 00014: val_f1 improved from 0.48811 to 0.50454, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1305 - f1: 0.4843 - val_loss: 0.1384 - val_f1: 0.5045\n",
            "Epoch 15/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1294 - f1: 0.4878\n",
            "Epoch 00015: val_f1 did not improve from 0.50454\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1293 - f1: 0.4879 - val_loss: 0.1422 - val_f1: 0.4899\n",
            "Epoch 16/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1277 - f1: 0.4983\n",
            "Epoch 00016: val_f1 did not improve from 0.50454\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1276 - f1: 0.4982 - val_loss: 0.1467 - val_f1: 0.4853\n",
            "Epoch 17/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1270 - f1: 0.5012\n",
            "Epoch 00017: val_f1 did not improve from 0.50454\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1270 - f1: 0.5011 - val_loss: 0.1430 - val_f1: 0.4683\n",
            "Epoch 18/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1258 - f1: 0.5100\n",
            "Epoch 00018: val_f1 did not improve from 0.50454\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1258 - f1: 0.5097 - val_loss: 0.1388 - val_f1: 0.4898\n",
            "Epoch 19/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1250 - f1: 0.5119\n",
            "Epoch 00019: val_f1 improved from 0.50454 to 0.51937, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1250 - f1: 0.5119 - val_loss: 0.1363 - val_f1: 0.5194\n",
            "Epoch 20/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1242 - f1: 0.5162\n",
            "Epoch 00020: val_f1 did not improve from 0.51937\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1241 - f1: 0.5160 - val_loss: 0.1419 - val_f1: 0.4849\n",
            "Epoch 21/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1232 - f1: 0.5211\n",
            "Epoch 00021: val_f1 did not improve from 0.51937\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1232 - f1: 0.5211 - val_loss: 0.1364 - val_f1: 0.4953\n",
            "Epoch 22/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1224 - f1: 0.5249\n",
            "Epoch 00022: val_f1 did not improve from 0.51937\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1223 - f1: 0.5249 - val_loss: 0.1375 - val_f1: 0.5044\n",
            "Epoch 23/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1218 - f1: 0.5274\n",
            "Epoch 00023: val_f1 did not improve from 0.51937\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1218 - f1: 0.5273 - val_loss: 0.1325 - val_f1: 0.5119\n",
            "Epoch 24/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1210 - f1: 0.5327\n",
            "Epoch 00024: val_f1 improved from 0.51937 to 0.54390, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1210 - f1: 0.5323 - val_loss: 0.1265 - val_f1: 0.5439\n",
            "Epoch 25/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1204 - f1: 0.5368\n",
            "Epoch 00025: val_f1 did not improve from 0.54390\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1204 - f1: 0.5368 - val_loss: 0.1379 - val_f1: 0.4759\n",
            "Epoch 26/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1197 - f1: 0.5395\n",
            "Epoch 00026: val_f1 did not improve from 0.54390\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1198 - f1: 0.5393 - val_loss: 0.1351 - val_f1: 0.5260\n",
            "Epoch 27/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1190 - f1: 0.5444\n",
            "Epoch 00027: val_f1 did not improve from 0.54390\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1190 - f1: 0.5440 - val_loss: 0.1371 - val_f1: 0.4786\n",
            "Epoch 28/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1184 - f1: 0.5450\n",
            "Epoch 00028: val_f1 did not improve from 0.54390\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1184 - f1: 0.5447 - val_loss: 0.1295 - val_f1: 0.5044\n",
            "Epoch 29/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1176 - f1: 0.5483\n",
            "Epoch 00029: val_f1 did not improve from 0.54390\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1176 - f1: 0.5480 - val_loss: 0.1327 - val_f1: 0.5172\n",
            "Epoch 30/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1170 - f1: 0.5517\n",
            "Epoch 00030: val_f1 did not improve from 0.54390\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1170 - f1: 0.5516 - val_loss: 0.1298 - val_f1: 0.5391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad14c6a7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy0HiK_U0jOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38b133ef-002d-402b-a2c7-d207c3b7a2b4"
      },
      "source": [
        "model_2.load_weights('model.weights')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fad99294ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMYHEHERAXac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model_2.predict(X_valid).reshape(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZHEIoQnAZyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b4275e08-5c24-4e48-ac3e-a1026b66755d"
      },
      "source": [
        "print(classification_report(y_valid, (preds > 0.5).astype(int)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97     61266\n",
            "           1       0.65      0.47      0.55      4041\n",
            "\n",
            "    accuracy                           0.95     65307\n",
            "   macro avg       0.81      0.73      0.76     65307\n",
            "weighted avg       0.95      0.95      0.95     65307\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLkQrLeZAyWq",
        "colab_type": "text"
      },
      "source": [
        "# Попытка 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z9ON9BiA0tM",
        "colab_type": "text"
      },
      "source": [
        "Попробуем учитывать регистр"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR4M6hjzAqOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_cased(text: str) -> list:\n",
        "  result = []\n",
        "  for ch in text:\n",
        "    if ch in ascii_letters or ch == ' ':\n",
        "      result.append(ch)\n",
        "    elif ch in digits:\n",
        "      result.append('DIG')\n",
        "    elif ch in short_punctuation:\n",
        "      result.append('PNC')\n",
        "    else:\n",
        "      result.append('UNK')\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9C3zvNnBFu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora['tokenized_cased'] = quora.cleaned.apply(tokenize_cased)\n",
        "\n",
        "vocab = build_vocab(quora.tokenized_cased.values, 1)\n",
        "\n",
        "char2id = index_chars(vocab)\n",
        "id2char = {i: ch for ch, i in char2id.items()}\n",
        "\n",
        "quora['indexed_cased'] = quora.tokenized_cased.apply(index_text)\n",
        "\n",
        "len_limit = limit_max_len(quora.indexed_cased.str.len(), 1)\n",
        "\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(quora.indexed_cased.values, maxlen=len_limit)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
        "                                                      test_size=0.05,\n",
        "                                                      random_state=1,\n",
        "                                                      stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRIlr4-ICohL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c373f843-b45c-44ba-f379-3406566b767b"
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(len_limit,))\n",
        "\n",
        "embeddings = tf.keras.layers.Embedding(input_dim=len(char2id), output_dim=50)(inputs)\n",
        "\n",
        "conv = []\n",
        "\n",
        "conv_1 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_1)\n",
        "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_2)\n",
        "pool_1 = tf.keras.layers.GlobalMaxPooling1D()(conv_3)\n",
        "drop_1 = tf.keras.layers.AlphaDropout(0.1)(pool_1)\n",
        "conv.append(drop_1)\n",
        "\n",
        "conv_4 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(embeddings)\n",
        "conv_5 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_4)\n",
        "conv_6 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_5)\n",
        "pool_2 = tf.keras.layers.GlobalMaxPooling1D()(conv_6)\n",
        "drop_2 = tf.keras.layers.AlphaDropout(0.1)(pool_2)\n",
        "conv.append(drop_2)\n",
        "\n",
        "conv_7 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(embeddings)\n",
        "conv_8 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_7)\n",
        "conv_9 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_8)\n",
        "pool_3 = tf.keras.layers.GlobalMaxPooling1D()(conv_9)\n",
        "drop_3 = tf.keras.layers.AlphaDropout(0.1)(pool_3)\n",
        "conv.append(drop_3)\n",
        "\n",
        "conv_10 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_11 = tf.keras.layers.Conv1D(kernel_size=2, filters=32, strides=1, activation='tanh')(conv_10)\n",
        "conv_12 = tf.keras.layers.Conv1D(kernel_size=2, filters=64, strides=1, activation='tanh')(conv_11)\n",
        "conv_13 = tf.keras.layers.Conv1D(kernel_size=2, filters=128, strides=1, activation='tanh')(conv_12)\n",
        "pool_4 = tf.keras.layers.GlobalMaxPooling1D()(conv_13)\n",
        "drop_4 = tf.keras.layers.AlphaDropout(0.1)(pool_4)\n",
        "conv.append(drop_4)\n",
        "\n",
        "concat = tf.keras.layers.Concatenate()(conv)\n",
        "dense_1 = tf.keras.layers.Dense(128, activation='selu')(concat)\n",
        "dense_2 = tf.keras.layers.Dense(64, activation='selu')(dense_1)\n",
        "dense_3 = tf.keras.layers.Dense(32, activation='selu')(dense_2)\n",
        "drop_5 = tf.keras.layers.AlphaDropout(0.1)(dense_3)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(drop_5)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.weights', \n",
        "                                                monitor='val_f1',\n",
        "                                                verbose=1,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                mode='max',\n",
        "                                                save_freq='epoch')\n",
        "\n",
        "model_3.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[f1])\n",
        "\n",
        "model_3.fit(X_train, y_train, \n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=4000,\n",
        "          epochs=30,\n",
        "          callbacks=[checkpoint])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.2255 - f1: 0.0026\n",
            "Epoch 00001: val_f1 improved from -inf to 0.00000, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.2254 - f1: 0.0026 - val_loss: 0.1981 - val_f1: 0.0000e+00\n",
            "Epoch 2/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1890 - f1: 0.0485\n",
            "Epoch 00002: val_f1 improved from 0.00000 to 0.31356, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1891 - f1: 0.0489 - val_loss: 0.1690 - val_f1: 0.3136\n",
            "Epoch 3/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1683 - f1: 0.2195\n",
            "Epoch 00003: val_f1 improved from 0.31356 to 0.33218, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1683 - f1: 0.2198 - val_loss: 0.1678 - val_f1: 0.3322\n",
            "Epoch 4/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1583 - f1: 0.3073\n",
            "Epoch 00004: val_f1 improved from 0.33218 to 0.36174, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1583 - f1: 0.3067 - val_loss: 0.1583 - val_f1: 0.3617\n",
            "Epoch 5/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1521 - f1: 0.3553\n",
            "Epoch 00005: val_f1 improved from 0.36174 to 0.45097, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1521 - f1: 0.3552 - val_loss: 0.1511 - val_f1: 0.4510\n",
            "Epoch 6/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1481 - f1: 0.3804\n",
            "Epoch 00006: val_f1 did not improve from 0.45097\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1481 - f1: 0.3805 - val_loss: 0.1519 - val_f1: 0.4434\n",
            "Epoch 7/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1454 - f1: 0.3962\n",
            "Epoch 00007: val_f1 did not improve from 0.45097\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1454 - f1: 0.3964 - val_loss: 0.1558 - val_f1: 0.3995\n",
            "Epoch 8/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1431 - f1: 0.4115\n",
            "Epoch 00008: val_f1 did not improve from 0.45097\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1431 - f1: 0.4113 - val_loss: 0.1506 - val_f1: 0.4117\n",
            "Epoch 9/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1413 - f1: 0.4228\n",
            "Epoch 00009: val_f1 did not improve from 0.45097\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1412 - f1: 0.4232 - val_loss: 0.1477 - val_f1: 0.4399\n",
            "Epoch 10/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1391 - f1: 0.4370\n",
            "Epoch 00010: val_f1 improved from 0.45097 to 0.45151, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1391 - f1: 0.4371 - val_loss: 0.1458 - val_f1: 0.4515\n",
            "Epoch 11/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1375 - f1: 0.4478\n",
            "Epoch 00011: val_f1 improved from 0.45151 to 0.50761, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1375 - f1: 0.4480 - val_loss: 0.1433 - val_f1: 0.5076\n",
            "Epoch 12/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1361 - f1: 0.4567\n",
            "Epoch 00012: val_f1 did not improve from 0.50761\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1360 - f1: 0.4565 - val_loss: 0.1432 - val_f1: 0.4462\n",
            "Epoch 13/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1343 - f1: 0.4649\n",
            "Epoch 00013: val_f1 did not improve from 0.50761\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1343 - f1: 0.4654 - val_loss: 0.1366 - val_f1: 0.5066\n",
            "Epoch 14/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1331 - f1: 0.4732\n",
            "Epoch 00014: val_f1 improved from 0.50761 to 0.53854, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1332 - f1: 0.4732 - val_loss: 0.1336 - val_f1: 0.5385\n",
            "Epoch 15/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1315 - f1: 0.4809\n",
            "Epoch 00015: val_f1 did not improve from 0.53854\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1315 - f1: 0.4811 - val_loss: 0.1367 - val_f1: 0.5039\n",
            "Epoch 16/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1304 - f1: 0.4879\n",
            "Epoch 00016: val_f1 did not improve from 0.53854\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1304 - f1: 0.4878 - val_loss: 0.1373 - val_f1: 0.4867\n",
            "Epoch 17/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1292 - f1: 0.4914\n",
            "Epoch 00017: val_f1 did not improve from 0.53854\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1292 - f1: 0.4913 - val_loss: 0.1339 - val_f1: 0.5222\n",
            "Epoch 18/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1284 - f1: 0.4977\n",
            "Epoch 00018: val_f1 did not improve from 0.53854\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1284 - f1: 0.4975 - val_loss: 0.1355 - val_f1: 0.5092\n",
            "Epoch 19/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1271 - f1: 0.5035\n",
            "Epoch 00019: val_f1 did not improve from 0.53854\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1271 - f1: 0.5029 - val_loss: 0.1344 - val_f1: 0.5140\n",
            "Epoch 20/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1261 - f1: 0.5086\n",
            "Epoch 00020: val_f1 did not improve from 0.53854\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1261 - f1: 0.5086 - val_loss: 0.1357 - val_f1: 0.5356\n",
            "Epoch 21/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1254 - f1: 0.5134\n",
            "Epoch 00021: val_f1 improved from 0.53854 to 0.54895, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1254 - f1: 0.5135 - val_loss: 0.1326 - val_f1: 0.5490\n",
            "Epoch 22/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1245 - f1: 0.5174\n",
            "Epoch 00022: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1246 - f1: 0.5177 - val_loss: 0.1290 - val_f1: 0.5450\n",
            "Epoch 23/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1236 - f1: 0.5222\n",
            "Epoch 00023: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1235 - f1: 0.5218 - val_loss: 0.1312 - val_f1: 0.5058\n",
            "Epoch 24/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1228 - f1: 0.5261\n",
            "Epoch 00024: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1228 - f1: 0.5256 - val_loss: 0.1344 - val_f1: 0.5072\n",
            "Epoch 25/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1223 - f1: 0.5276\n",
            "Epoch 00025: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1223 - f1: 0.5272 - val_loss: 0.1399 - val_f1: 0.4727\n",
            "Epoch 26/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1213 - f1: 0.5342\n",
            "Epoch 00026: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1213 - f1: 0.5338 - val_loss: 0.1394 - val_f1: 0.4745\n",
            "Epoch 27/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1204 - f1: 0.5386\n",
            "Epoch 00027: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1204 - f1: 0.5385 - val_loss: 0.1318 - val_f1: 0.5323\n",
            "Epoch 28/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1200 - f1: 0.5398\n",
            "Epoch 00028: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1201 - f1: 0.5395 - val_loss: 0.1393 - val_f1: 0.4778\n",
            "Epoch 29/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1194 - f1: 0.5420\n",
            "Epoch 00029: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1193 - f1: 0.5422 - val_loss: 0.1323 - val_f1: 0.5402\n",
            "Epoch 30/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1187 - f1: 0.5467\n",
            "Epoch 00030: val_f1 did not improve from 0.54895\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1186 - f1: 0.5470 - val_loss: 0.1383 - val_f1: 0.5034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fae523f3400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgHM_neuECmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bbb24879-5897-44b3-8027-c74f7908a9f7"
      },
      "source": [
        "model_3.load_weights('model.weights')\n",
        "preds = model_3.predict(X_valid).reshape(-1)\n",
        "print(classification_report(y_valid, (preds > 0.5).astype(int)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97     61266\n",
            "           1       0.64      0.48      0.55      4041\n",
            "\n",
            "    accuracy                           0.95     65307\n",
            "   macro avg       0.80      0.73      0.76     65307\n",
            "weighted avg       0.95      0.95      0.95     65307\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YU5NejyJKyz",
        "colab_type": "text"
      },
      "source": [
        "Здесь разница только в том, что пик ф-меры был достигнут на на несколько эпох раньше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1USsf2ewH0O9",
        "colab_type": "text"
      },
      "source": [
        "# Попытка 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STmnuvWxH23O",
        "colab_type": "text"
      },
      "source": [
        "Оставляем регистр, добавляем в словарь \"значимую\" пунктуацию. Остальную храним как один символ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBQU9uhFJX1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "long_punctuation = set(punctuation) - set(short_punctuation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Divy0eH2Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_cased_punct(text: str) -> list:\n",
        "  result = []\n",
        "  for ch in text:\n",
        "    if ch in ascii_letters or ch in short_punctuation or ch == ' ':\n",
        "      result.append(ch)\n",
        "    elif ch in digits:\n",
        "      result.append('DIG')\n",
        "    elif ch in long_punctuation:\n",
        "      result.append('PNC')\n",
        "    else:\n",
        "      result.append('UNK')\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek-XGh3wIMtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora['tokenized_cased_punct'] = quora.cleaned.apply(tokenize_cased)\n",
        "\n",
        "vocab = build_vocab(quora.tokenized_cased_punct.values, 1)\n",
        "\n",
        "char2id = index_chars(vocab)\n",
        "id2char = {i: ch for ch, i in char2id.items()}\n",
        "\n",
        "quora['indexed_cased_punct'] = quora.tokenized_cased_punct.apply(index_text)\n",
        "\n",
        "len_limit = limit_max_len(quora.indexed_cased_punct.str.len(), 1)\n",
        "\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(quora.indexed_cased_punct.values, maxlen=len_limit)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
        "                                                      test_size=0.05,\n",
        "                                                      random_state=1,\n",
        "                                                      stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_J9vh4cIMwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a02760a8-d84a-4621-9a40-7e800ba9031a"
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(len_limit,))\n",
        "\n",
        "embeddings = tf.keras.layers.Embedding(input_dim=len(char2id), output_dim=50)(inputs)\n",
        "\n",
        "conv = []\n",
        "\n",
        "conv_1 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_1)\n",
        "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_2)\n",
        "pool_1 = tf.keras.layers.GlobalMaxPooling1D()(conv_3)\n",
        "drop_1 = tf.keras.layers.AlphaDropout(0.1)(pool_1)\n",
        "conv.append(drop_1)\n",
        "\n",
        "conv_4 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(embeddings)\n",
        "conv_5 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_4)\n",
        "conv_6 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_5)\n",
        "pool_2 = tf.keras.layers.GlobalMaxPooling1D()(conv_6)\n",
        "drop_2 = tf.keras.layers.AlphaDropout(0.1)(pool_2)\n",
        "conv.append(drop_2)\n",
        "\n",
        "conv_7 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(embeddings)\n",
        "conv_8 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_7)\n",
        "conv_9 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_8)\n",
        "pool_3 = tf.keras.layers.GlobalMaxPooling1D()(conv_9)\n",
        "drop_3 = tf.keras.layers.AlphaDropout(0.1)(pool_3)\n",
        "conv.append(drop_3)\n",
        "\n",
        "conv_10 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_11 = tf.keras.layers.Conv1D(kernel_size=2, filters=32, strides=1, activation='tanh')(conv_10)\n",
        "conv_12 = tf.keras.layers.Conv1D(kernel_size=2, filters=64, strides=1, activation='tanh')(conv_11)\n",
        "conv_13 = tf.keras.layers.Conv1D(kernel_size=2, filters=128, strides=1, activation='tanh')(conv_12)\n",
        "pool_4 = tf.keras.layers.GlobalMaxPooling1D()(conv_13)\n",
        "drop_4 = tf.keras.layers.AlphaDropout(0.1)(pool_4)\n",
        "conv.append(drop_4)\n",
        "\n",
        "concat = tf.keras.layers.Concatenate()(conv)\n",
        "dense_1 = tf.keras.layers.Dense(128, activation='selu')(concat)\n",
        "dense_2 = tf.keras.layers.Dense(64, activation='selu')(dense_1)\n",
        "dense_3 = tf.keras.layers.Dense(32, activation='selu')(dense_2)\n",
        "drop_5 = tf.keras.layers.AlphaDropout(0.1)(dense_3)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(drop_5)\n",
        "\n",
        "model_4 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.weights', \n",
        "                                                monitor='val_f1',\n",
        "                                                verbose=1,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                mode='max',\n",
        "                                                save_freq='epoch')\n",
        "\n",
        "model_4.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[f1])\n",
        "\n",
        "model_4.fit(X_train, y_train, \n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=4000,\n",
        "          epochs=30,\n",
        "          callbacks=[checkpoint])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.2214 - f1: 0.0023\n",
            "Epoch 00001: val_f1 improved from -inf to 0.00000, saving model to model.weights\n",
            "311/311 [==============================] - 40s 130ms/step - loss: 0.2214 - f1: 0.0023 - val_loss: 0.1971 - val_f1: 0.0000e+00\n",
            "Epoch 2/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1888 - f1: 0.0490\n",
            "Epoch 00002: val_f1 improved from 0.00000 to 0.28070, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1888 - f1: 0.0493 - val_loss: 0.1714 - val_f1: 0.2807\n",
            "Epoch 3/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1689 - f1: 0.2136\n",
            "Epoch 00003: val_f1 improved from 0.28070 to 0.31928, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1689 - f1: 0.2140 - val_loss: 0.1606 - val_f1: 0.3193\n",
            "Epoch 4/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1579 - f1: 0.3047\n",
            "Epoch 00004: val_f1 improved from 0.31928 to 0.37249, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1579 - f1: 0.3048 - val_loss: 0.1523 - val_f1: 0.3725\n",
            "Epoch 5/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1519 - f1: 0.3519\n",
            "Epoch 00005: val_f1 improved from 0.37249 to 0.40427, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1519 - f1: 0.3520 - val_loss: 0.1531 - val_f1: 0.4043\n",
            "Epoch 6/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1484 - f1: 0.3744\n",
            "Epoch 00006: val_f1 improved from 0.40427 to 0.40608, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1484 - f1: 0.3740 - val_loss: 0.1486 - val_f1: 0.4061\n",
            "Epoch 7/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1447 - f1: 0.4022\n",
            "Epoch 00007: val_f1 improved from 0.40608 to 0.40639, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1446 - f1: 0.4023 - val_loss: 0.1469 - val_f1: 0.4064\n",
            "Epoch 8/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1419 - f1: 0.4146\n",
            "Epoch 00008: val_f1 improved from 0.40639 to 0.48436, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1418 - f1: 0.4149 - val_loss: 0.1480 - val_f1: 0.4844\n",
            "Epoch 9/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1397 - f1: 0.4324\n",
            "Epoch 00009: val_f1 improved from 0.48436 to 0.51356, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1397 - f1: 0.4327 - val_loss: 0.1404 - val_f1: 0.5136\n",
            "Epoch 10/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1377 - f1: 0.4437\n",
            "Epoch 00010: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1377 - f1: 0.4434 - val_loss: 0.1417 - val_f1: 0.4646\n",
            "Epoch 11/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1355 - f1: 0.4582\n",
            "Epoch 00011: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1355 - f1: 0.4582 - val_loss: 0.1371 - val_f1: 0.5046\n",
            "Epoch 12/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1336 - f1: 0.4645\n",
            "Epoch 00012: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1335 - f1: 0.4648 - val_loss: 0.1348 - val_f1: 0.5069\n",
            "Epoch 13/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1323 - f1: 0.4750\n",
            "Epoch 00013: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1323 - f1: 0.4747 - val_loss: 0.1400 - val_f1: 0.4661\n",
            "Epoch 14/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1310 - f1: 0.4816\n",
            "Epoch 00014: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1310 - f1: 0.4812 - val_loss: 0.1349 - val_f1: 0.4950\n",
            "Epoch 15/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1292 - f1: 0.4936\n",
            "Epoch 00015: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1292 - f1: 0.4936 - val_loss: 0.1378 - val_f1: 0.5009\n",
            "Epoch 16/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1283 - f1: 0.4950\n",
            "Epoch 00016: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1282 - f1: 0.4948 - val_loss: 0.1412 - val_f1: 0.4919\n",
            "Epoch 17/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1273 - f1: 0.5023\n",
            "Epoch 00017: val_f1 did not improve from 0.51356\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1273 - f1: 0.5022 - val_loss: 0.1419 - val_f1: 0.4883\n",
            "Epoch 18/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1260 - f1: 0.5070\n",
            "Epoch 00018: val_f1 improved from 0.51356 to 0.51943, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1260 - f1: 0.5072 - val_loss: 0.1354 - val_f1: 0.5194\n",
            "Epoch 19/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1251 - f1: 0.5131\n",
            "Epoch 00019: val_f1 improved from 0.51943 to 0.52705, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1250 - f1: 0.5130 - val_loss: 0.1322 - val_f1: 0.5270\n",
            "Epoch 20/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1241 - f1: 0.5179\n",
            "Epoch 00020: val_f1 did not improve from 0.52705\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1241 - f1: 0.5179 - val_loss: 0.1314 - val_f1: 0.5270\n",
            "Epoch 21/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1232 - f1: 0.5226\n",
            "Epoch 00021: val_f1 improved from 0.52705 to 0.52741, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1233 - f1: 0.5227 - val_loss: 0.1303 - val_f1: 0.5274\n",
            "Epoch 22/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1225 - f1: 0.5256\n",
            "Epoch 00022: val_f1 improved from 0.52741 to 0.53304, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.1224 - f1: 0.5255 - val_loss: 0.1332 - val_f1: 0.5330\n",
            "Epoch 23/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1216 - f1: 0.5307\n",
            "Epoch 00023: val_f1 did not improve from 0.53304\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1216 - f1: 0.5305 - val_loss: 0.1347 - val_f1: 0.5222\n",
            "Epoch 24/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1209 - f1: 0.5351\n",
            "Epoch 00024: val_f1 did not improve from 0.53304\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1210 - f1: 0.5348 - val_loss: 0.1340 - val_f1: 0.5234\n",
            "Epoch 25/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1203 - f1: 0.5379\n",
            "Epoch 00025: val_f1 did not improve from 0.53304\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1203 - f1: 0.5378 - val_loss: 0.1307 - val_f1: 0.5232\n",
            "Epoch 26/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1194 - f1: 0.5432\n",
            "Epoch 00026: val_f1 improved from 0.53304 to 0.55654, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1195 - f1: 0.5432 - val_loss: 0.1290 - val_f1: 0.5565\n",
            "Epoch 27/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1189 - f1: 0.5458\n",
            "Epoch 00027: val_f1 did not improve from 0.55654\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1190 - f1: 0.5459 - val_loss: 0.1301 - val_f1: 0.5324\n",
            "Epoch 28/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1183 - f1: 0.5474\n",
            "Epoch 00028: val_f1 did not improve from 0.55654\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1183 - f1: 0.5473 - val_loss: 0.1288 - val_f1: 0.5367\n",
            "Epoch 29/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1176 - f1: 0.5535\n",
            "Epoch 00029: val_f1 did not improve from 0.55654\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1175 - f1: 0.5534 - val_loss: 0.1376 - val_f1: 0.4967\n",
            "Epoch 30/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1167 - f1: 0.5567\n",
            "Epoch 00030: val_f1 did not improve from 0.55654\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1168 - f1: 0.5568 - val_loss: 0.1327 - val_f1: 0.5171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fabc94c8b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPxysarNIMys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "eb6cffac-711a-435d-8080-cabcfb24cdd1"
      },
      "source": [
        "model_4.load_weights('model.weights')\n",
        "preds = model_4.predict(X_valid).reshape(-1)\n",
        "print(classification_report(y_valid, (preds > 0.5).astype(int)))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97     61266\n",
            "           1       0.64      0.49      0.56      4041\n",
            "\n",
            "    accuracy                           0.95     65307\n",
            "   macro avg       0.81      0.74      0.77     65307\n",
            "weighted avg       0.95      0.95      0.95     65307\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g87cUFyIO84p",
        "colab_type": "text"
      },
      "source": [
        "Как видим, качество несколько улучшилось. Вероятно в пунктуации действительно содержится полезная информация для нашего классификатора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM7vT2o-INPY",
        "colab_type": "text"
      },
      "source": [
        "# Попытка 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghNSCA6cK0bY",
        "colab_type": "text"
      },
      "source": [
        "Оставляем только латинские буквы нижнего регистра"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzZ85kNjIO2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_ascii(text: str) -> list:\n",
        "  result = []\n",
        "  for ch in text.lower():\n",
        "    if ch in ascii_lower:\n",
        "      result.append(ch)\n",
        "    else:\n",
        "      result.append('UNK')\n",
        "  return result\n",
        "\n",
        "def index_chars(chars: set) -> dict:\n",
        "  d = {'UNK': 1, 'PAD': 0}\n",
        "  for ch in chars :\n",
        "    if ch not in d.keys():\n",
        "      d[ch] = len(d)\n",
        "  return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuxW8KntLDPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora['tokenized_ascii'] = quora.cleaned.apply(tokenize_cased)\n",
        "\n",
        "vocab = build_vocab(quora.tokenized_ascii.values, 1)\n",
        "\n",
        "char2id = index_chars(vocab)\n",
        "id2char = {i: ch for ch, i in char2id.items()}\n",
        "\n",
        "quora['indexed_ascii'] = quora.tokenized_ascii.apply(index_text)\n",
        "\n",
        "len_limit = limit_max_len(quora.indexed_ascii.str.len(), 1)\n",
        "\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(quora.indexed_ascii.values, maxlen=len_limit)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
        "                                                      test_size=0.05,\n",
        "                                                      random_state=1,\n",
        "                                                      stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS3F8ydBLvRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "748dfd59-89dd-4103-cc39-95dbb37adeae"
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(len_limit,))\n",
        "\n",
        "embeddings = tf.keras.layers.Embedding(input_dim=len(char2id), output_dim=50)(inputs)\n",
        "\n",
        "conv = []\n",
        "\n",
        "conv_1 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_2 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_1)\n",
        "conv_3 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(conv_2)\n",
        "pool_1 = tf.keras.layers.GlobalMaxPooling1D()(conv_3)\n",
        "drop_1 = tf.keras.layers.AlphaDropout(0.1)(pool_1)\n",
        "conv.append(drop_1)\n",
        "\n",
        "conv_4 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(embeddings)\n",
        "conv_5 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_4)\n",
        "conv_6 = tf.keras.layers.Conv1D(kernel_size=3, filters=32, strides=1, activation='tanh')(conv_5)\n",
        "pool_2 = tf.keras.layers.GlobalMaxPooling1D()(conv_6)\n",
        "drop_2 = tf.keras.layers.AlphaDropout(0.1)(pool_2)\n",
        "conv.append(drop_2)\n",
        "\n",
        "conv_7 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(embeddings)\n",
        "conv_8 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_7)\n",
        "conv_9 = tf.keras.layers.Conv1D(kernel_size=4, filters=64, strides=1, activation='tanh')(conv_8)\n",
        "pool_3 = tf.keras.layers.GlobalMaxPooling1D()(conv_9)\n",
        "drop_3 = tf.keras.layers.AlphaDropout(0.1)(pool_3)\n",
        "conv.append(drop_3)\n",
        "\n",
        "conv_10 = tf.keras.layers.Conv1D(kernel_size=2, filters=16, strides=1, activation='tanh')(embeddings)\n",
        "conv_11 = tf.keras.layers.Conv1D(kernel_size=2, filters=32, strides=1, activation='tanh')(conv_10)\n",
        "conv_12 = tf.keras.layers.Conv1D(kernel_size=2, filters=64, strides=1, activation='tanh')(conv_11)\n",
        "conv_13 = tf.keras.layers.Conv1D(kernel_size=2, filters=128, strides=1, activation='tanh')(conv_12)\n",
        "pool_4 = tf.keras.layers.GlobalMaxPooling1D()(conv_13)\n",
        "drop_4 = tf.keras.layers.AlphaDropout(0.1)(pool_4)\n",
        "conv.append(drop_4)\n",
        "\n",
        "concat = tf.keras.layers.Concatenate()(conv)\n",
        "dense_1 = tf.keras.layers.Dense(128, activation='selu')(concat)\n",
        "dense_2 = tf.keras.layers.Dense(64, activation='selu')(dense_1)\n",
        "dense_3 = tf.keras.layers.Dense(32, activation='selu')(dense_2)\n",
        "drop_5 = tf.keras.layers.AlphaDropout(0.1)(dense_3)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(drop_5)\n",
        "\n",
        "model_5 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.weights', \n",
        "                                                monitor='val_f1',\n",
        "                                                verbose=1,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                mode='max',\n",
        "                                                save_freq='epoch')\n",
        "\n",
        "model_5.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[f1])\n",
        "\n",
        "model_5.fit(X_train, y_train, \n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=4000,\n",
        "          epochs=30,\n",
        "          callbacks=[checkpoint])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.2273 - f1: 0.0045\n",
            "Epoch 00001: val_f1 improved from -inf to 0.00000, saving model to model.weights\n",
            "311/311 [==============================] - 40s 129ms/step - loss: 0.2272 - f1: 0.0044 - val_loss: 0.1958 - val_f1: 0.0000e+00\n",
            "Epoch 2/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1828 - f1: 0.1001\n",
            "Epoch 00002: val_f1 improved from 0.00000 to 0.34488, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1827 - f1: 0.1006 - val_loss: 0.1705 - val_f1: 0.3449\n",
            "Epoch 3/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1634 - f1: 0.2700\n",
            "Epoch 00003: val_f1 did not improve from 0.34488\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1634 - f1: 0.2700 - val_loss: 0.1631 - val_f1: 0.3214\n",
            "Epoch 4/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1546 - f1: 0.3337\n",
            "Epoch 00004: val_f1 improved from 0.34488 to 0.44221, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1546 - f1: 0.3342 - val_loss: 0.1500 - val_f1: 0.4422\n",
            "Epoch 5/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1502 - f1: 0.3643\n",
            "Epoch 00005: val_f1 improved from 0.44221 to 0.44640, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1502 - f1: 0.3646 - val_loss: 0.1486 - val_f1: 0.4464\n",
            "Epoch 6/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1467 - f1: 0.3904\n",
            "Epoch 00006: val_f1 improved from 0.44640 to 0.45315, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1468 - f1: 0.3904 - val_loss: 0.1447 - val_f1: 0.4531\n",
            "Epoch 7/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1439 - f1: 0.4050\n",
            "Epoch 00007: val_f1 improved from 0.45315 to 0.46922, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1439 - f1: 0.4052 - val_loss: 0.1431 - val_f1: 0.4692\n",
            "Epoch 8/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1412 - f1: 0.4249\n",
            "Epoch 00008: val_f1 improved from 0.46922 to 0.47121, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1411 - f1: 0.4254 - val_loss: 0.1442 - val_f1: 0.4712\n",
            "Epoch 9/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1389 - f1: 0.4370\n",
            "Epoch 00009: val_f1 improved from 0.47121 to 0.50605, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1390 - f1: 0.4374 - val_loss: 0.1393 - val_f1: 0.5060\n",
            "Epoch 10/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1367 - f1: 0.4474\n",
            "Epoch 00010: val_f1 did not improve from 0.50605\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1367 - f1: 0.4476 - val_loss: 0.1430 - val_f1: 0.4710\n",
            "Epoch 11/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1351 - f1: 0.4580\n",
            "Epoch 00011: val_f1 did not improve from 0.50605\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1351 - f1: 0.4581 - val_loss: 0.1418 - val_f1: 0.4611\n",
            "Epoch 12/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1333 - f1: 0.4650\n",
            "Epoch 00012: val_f1 did not improve from 0.50605\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1333 - f1: 0.4655 - val_loss: 0.1492 - val_f1: 0.4153\n",
            "Epoch 13/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1322 - f1: 0.4721\n",
            "Epoch 00013: val_f1 improved from 0.50605 to 0.53679, saving model to model.weights\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1323 - f1: 0.4719 - val_loss: 0.1325 - val_f1: 0.5368\n",
            "Epoch 14/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1310 - f1: 0.4779\n",
            "Epoch 00014: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1308 - f1: 0.4780 - val_loss: 0.1385 - val_f1: 0.5135\n",
            "Epoch 15/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1294 - f1: 0.4890\n",
            "Epoch 00015: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1294 - f1: 0.4890 - val_loss: 0.1417 - val_f1: 0.4533\n",
            "Epoch 16/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1284 - f1: 0.4922\n",
            "Epoch 00016: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1284 - f1: 0.4925 - val_loss: 0.1474 - val_f1: 0.4353\n",
            "Epoch 17/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1274 - f1: 0.4972\n",
            "Epoch 00017: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1275 - f1: 0.4972 - val_loss: 0.1303 - val_f1: 0.4992\n",
            "Epoch 18/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1264 - f1: 0.5036\n",
            "Epoch 00018: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 127ms/step - loss: 0.1264 - f1: 0.5038 - val_loss: 0.1439 - val_f1: 0.4453\n",
            "Epoch 19/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1254 - f1: 0.5096\n",
            "Epoch 00019: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 127ms/step - loss: 0.1255 - f1: 0.5091 - val_loss: 0.1379 - val_f1: 0.4554\n",
            "Epoch 20/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1247 - f1: 0.5123\n",
            "Epoch 00020: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 127ms/step - loss: 0.1247 - f1: 0.5121 - val_loss: 0.1392 - val_f1: 0.4865\n",
            "Epoch 21/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1235 - f1: 0.5189\n",
            "Epoch 00021: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1235 - f1: 0.5191 - val_loss: 0.1357 - val_f1: 0.5245\n",
            "Epoch 22/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1228 - f1: 0.5228\n",
            "Epoch 00022: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1229 - f1: 0.5229 - val_loss: 0.1318 - val_f1: 0.5263\n",
            "Epoch 23/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1220 - f1: 0.5279\n",
            "Epoch 00023: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1219 - f1: 0.5285 - val_loss: 0.1398 - val_f1: 0.4868\n",
            "Epoch 24/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1214 - f1: 0.5305\n",
            "Epoch 00024: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1214 - f1: 0.5303 - val_loss: 0.1317 - val_f1: 0.5105\n",
            "Epoch 25/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1206 - f1: 0.5341\n",
            "Epoch 00025: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1206 - f1: 0.5341 - val_loss: 0.1319 - val_f1: 0.5295\n",
            "Epoch 26/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1197 - f1: 0.5407\n",
            "Epoch 00026: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1197 - f1: 0.5406 - val_loss: 0.1289 - val_f1: 0.5019\n",
            "Epoch 27/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1192 - f1: 0.5419\n",
            "Epoch 00027: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1192 - f1: 0.5420 - val_loss: 0.1277 - val_f1: 0.5259\n",
            "Epoch 28/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1184 - f1: 0.5468\n",
            "Epoch 00028: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1185 - f1: 0.5465 - val_loss: 0.1344 - val_f1: 0.5094\n",
            "Epoch 29/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1179 - f1: 0.5474\n",
            "Epoch 00029: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1179 - f1: 0.5474 - val_loss: 0.1394 - val_f1: 0.4875\n",
            "Epoch 30/30\n",
            "310/311 [============================>.] - ETA: 0s - loss: 0.1172 - f1: 0.5514\n",
            "Epoch 00030: val_f1 did not improve from 0.53679\n",
            "311/311 [==============================] - 40s 128ms/step - loss: 0.1172 - f1: 0.5513 - val_loss: 0.1331 - val_f1: 0.5168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fab7cc813c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpG6ImVNL0IB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a686f6b6-9bee-45b9-98f0-6686137867a1"
      },
      "source": [
        "model_5.load_weights('model.weights')\n",
        "preds = model_5.predict(X_valid).reshape(-1)\n",
        "print(classification_report(y_valid, (preds > 0.5).astype(int)))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97     61266\n",
            "           1       0.62      0.47      0.54      4041\n",
            "\n",
            "    accuracy                           0.95     65307\n",
            "   macro avg       0.80      0.73      0.76     65307\n",
            "weighted avg       0.94      0.95      0.95     65307\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu2qBkPFUEvX",
        "colab_type": "text"
      },
      "source": [
        "Качество ухудшилось, но не так уж сильно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP3ADhXbUKfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}